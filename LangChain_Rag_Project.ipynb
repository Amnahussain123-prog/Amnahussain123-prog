{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QnSgoXKA48p-TaAWYbGd0R-DYkIibaGq",
      "authorship_tag": "ABX9TyMm8zHWhuwEnOnAFN6b/JBv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amnahussain123-prog/Amnahussain123-prog/blob/main/LangChain_Rag_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-pinecone langchain-google-genai"
      ],
      "metadata": {
        "id": "FmluI0pqOJrK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "api_key=\"your_pinecone_api_key\"\n",
        "environment=\"your_pinecone_environment\"\n",
        "google_gemini_api_key=\"your_google_gemini_api_key\"\n",
        "\n",
        "api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "environment = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
        "google_gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "J-L4OgbCVN-5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "api_key = userdata.get('PINECONE_API_KEY')\n",
        "environment=os.getenv(\"PINECONE_ENVIRONMENT\")\n",
        "\n",
        "pc = Pinecone(\n",
        "        api_key=userdata.get(\"PINECONE_API_KEY\"),\n",
        "        environment=os.getenv(\"PINECONE_ENVIRONMENT\")\n",
        "    )\n"
      ],
      "metadata": {
        "id": "D6Sul7SzmyQD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  -qU --upgrade pinecone-client"
      ],
      "metadata": {
        "id": "vczCgIBh1RMi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = 'gemini-rag-index'\n",
        "# Delete the existing index (if it exists)\n",
        "try:\n",
        "    pc.delete_index(index_name)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create a new index with the correct dimension\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,  # Change dimension to 768\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n"
      ],
      "metadata": {
        "id": "_QGH7eSvYEoo"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
      ],
      "metadata": {
        "id": "QwTlY2KR2EiR"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"i'm a pre-medical student.\")\n",
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P7e6zLm3WEy",
        "outputId": "056afb15-699e-4136-9c46-95539ba2b5e0"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.027932917699217796,\n",
              " -0.06260764598846436,\n",
              " 0.012762178666889668,\n",
              " -0.026814168319106102,\n",
              " 0.019282404333353043]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-chroma\n",
        "!pip install langchain_openai\n",
        "!pip install -Uq langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sKM8kAiB54K",
        "outputId": "6a6aa4b6-c385-42f6-e97c-d8cc67fe5202"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.29)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.59.4)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_openai) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_openai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "c-M9l3G9CByv"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "CRDQyMk3dIGF"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain_community"
      ],
      "metadata": {
        "id": "5JKSBFZTenCP"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "docs_1 = Document (page_content=f\"\"\"\n",
        "Pollution has tremendous adverse affects on environment, resulting in its degradation and also in depletion of living species. It has severe aftereffects like acid rain, global warming, famine, draught and extreme climatic conditions along with other consequences. The most common type of pollution is air pollution.\n",
        "\"\"\", metadata={\"source\": \"local\"})\n",
        "\n",
        "docs_2 = Document (page_content=f\"\"\"\n",
        "Global Warming is a dangerous effect on our environment that we are facing these days. Rapid industrialization, increase in the population growth and pollution are causing a rise in Global Warming. Global Warming refers to the increase in the average temperature of the earth's surface during the last century.\n",
        "\"\"\", metadata={\"source\": \"local\"})\n",
        "\n",
        "docs_3 = Document (page_content= f\"\"\"\n",
        "Artificial intelligence is a field of science concerned with building computers and machines that can reason, learn, and act in such a way that would normally require human intelligence or that involves data whose scale exceeds what humans can analyze.\n",
        "\"\"\", metadata={\"source\": \"local\"})\n",
        "\n",
        "docs_4 = Document(page_content=f\"\"\"\n",
        "Agentic AI revolutionizes customer interactions by providing personalized and responsive experiences at scale and speed. Leveraging sophisticated models, AI agents can infer customer intent, predict needs, and offer tailored solutions, all while operating 24/7 to ensure consistent and efficient support.\n",
        "\"\"\", metadata={\"source\": \"local\"})\n",
        "\n",
        "docs_5 = Document(page_content=f\"\"\"\n",
        "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response.\n",
        "\"\"\", metadata={\"Source\": \"local\"})\n",
        "loader = Document(\"documents.txt\")  # Replace with your file\n",
        "\n",
        "# Load documents\n",
        "loader = TextLoader(\"documents.txt\")  # Replace with your file\n",
        "# DOCS = text_loader.load_documents([docs_1, docs_2, docs_3, docs_4, docs_5]) # text_loader is not defined, use loader instead, and it's unnecessary here\n",
        "# DOCS = loader.load() # Uncomment this line if you want to load documents from a file\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "DOCS = text_splitter.split_documents([docs_1, docs_2, docs_3, docs_4, docs_5])\n",
        "\n",
        "DOCS = [docs_1, docs_2, docs_3, docs_4, docs_5]"
      ],
      "metadata": {
        "id": "0UQq0ITOdNfL"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"your document content 1\", metadata={\"source\": \"local\"}),\n",
        "    Document(page_content=\"your document content 2\", metadata={\"source\": \"local\"}),\n",
        "    Document(page_content=\"your document content 3\", metadata={\"source\": \"local\"}),\n",
        "    Document(page_content=\"your document content 4\", metadata={\"source\": \"local\"}),\n",
        "    Document(page_content=\"your document content 5\", metadata={\"source\": \"local\"})\n",
        "]\n",
        "\n",
        "# Pass the 'documents' list instead of the 'Document' class\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents,  # Pass the list of documents here\n",
        "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        ")"
      ],
      "metadata": {
        "id": "sDTg8BVcCUY3"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(dir(vectorstore))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe_PgdHwExqu",
        "outputId": "2f7978d9-53dd-465c-cef3-06dca1219098"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_Chroma__ensure_collection',\n",
              " '_Chroma__query_collection',\n",
              " '_LANGCHAIN_DEFAULT_COLLECTION_NAME',\n",
              " '__abstractmethods__',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_asimilarity_search_with_relevance_scores',\n",
              " '_chroma_collection',\n",
              " '_client',\n",
              " '_client_settings',\n",
              " '_collection',\n",
              " '_collection_metadata',\n",
              " '_collection_name',\n",
              " '_cosine_relevance_score_fn',\n",
              " '_embedding_function',\n",
              " '_euclidean_relevance_score_fn',\n",
              " '_get_retriever_tags',\n",
              " '_max_inner_product_relevance_score_fn',\n",
              " '_persist_directory',\n",
              " '_select_relevance_score_fn',\n",
              " '_similarity_search_with_relevance_scores',\n",
              " 'aadd_documents',\n",
              " 'aadd_texts',\n",
              " 'add_documents',\n",
              " 'add_images',\n",
              " 'add_texts',\n",
              " 'adelete',\n",
              " 'afrom_documents',\n",
              " 'afrom_texts',\n",
              " 'aget_by_ids',\n",
              " 'amax_marginal_relevance_search',\n",
              " 'amax_marginal_relevance_search_by_vector',\n",
              " 'as_retriever',\n",
              " 'asearch',\n",
              " 'asimilarity_search',\n",
              " 'asimilarity_search_by_vector',\n",
              " 'asimilarity_search_with_relevance_scores',\n",
              " 'asimilarity_search_with_score',\n",
              " 'delete',\n",
              " 'delete_collection',\n",
              " 'embeddings',\n",
              " 'encode_image',\n",
              " 'from_documents',\n",
              " 'from_texts',\n",
              " 'get',\n",
              " 'get_by_ids',\n",
              " 'max_marginal_relevance_search',\n",
              " 'max_marginal_relevance_search_by_vector',\n",
              " 'override_relevance_score_fn',\n",
              " 'reset_collection',\n",
              " 'search',\n",
              " 'similarity_search',\n",
              " 'similarity_search_by_image',\n",
              " 'similarity_search_by_image_with_relevance_score',\n",
              " 'similarity_search_by_vector',\n",
              " 'similarity_search_by_vector_with_relevance_scores',\n",
              " 'similarity_search_with_relevance_scores',\n",
              " 'similarity_search_with_score',\n",
              " 'update_document',\n",
              " 'update_documents']"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "for doc in tqdm(DOCS):\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "    vectorstore.add_documents(documents=[doc], embedding=embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0fn78BE-pQ4",
        "outputId": "ebe8f7fe-ce5c-428b-8e76-e722029c7f88"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  2.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone  # Import from langchain_pinecone\n",
        "# ... (other imports)\n",
        "\n",
        "# Instantiate the PineconeVectorStore with the correct arguments\n",
        "retriever = PineconeVectorStore(\n",
        "    index=index,  # Your Pinecone index object\n",
        "    embedding=embeddings  # Your embedding function\n",
        ")"
      ],
      "metadata": {
        "id": "wjH2rzraFVG4"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-google-genai"
      ],
      "metadata": {
        "id": "Sf2H1ofGF22G"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI # Import ChatGoogleGenerativeAI instead of GoogleGeminiFlash\n",
        "\n",
        "gemini_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",  # Example: using \"gemini-pro\" model\n",
        "    google_gemini_api_key = 'GOOGLE_API_KEY',\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "aIRQdVImHp05"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import Pinecone\n",
        "\n",
        "# Update retriever to use Pinecone class from langchain_pinecone\n",
        "# Notice the changed class name: Pinecone -> PineconeVectorStore\n",
        "retriever = PineconeVectorStore(index=index, embedding=embeddings)\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "retriever = vector_store.as_retriever() # Get retriever from vector_store\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gemini_model,\n",
        "    chain_type=\"stuff\",  # Other options: \"map_reduce\", \"refine\"\n",
        "    retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "G_MFIC59IVlj"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=gemini_model, prompt=prompt_template)\n",
        "question = \"What were the major milestones in AI development during the 20th century?\"\n",
        "response = chain.run(question)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL3R_zTlgUX5",
        "outputId": "9b8ca518-97c4-49b5-c7bc-5e02a8885e3d"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**1943:** Warren McCulloch and Walter Pitts develop a mathematical model of a neuron, providing a foundation for understanding artificial neural networks.\n",
            "\n",
            "**1950:** Alan Turing publishes \"Computing Machinery and Intelligence,\" proposing the Turing Test as a measure of machine intelligence.\n",
            "\n",
            "**1956:** The Dartmouth Summer Research Project on Artificial Intelligence is held, coining the term \"artificial intelligence.\"\n",
            "\n",
            "**1957:** Frank Rosenblatt develops the perceptron, an early artificial neural network.\n",
            "\n",
            "**1965:** Joseph Weizenbaum creates ELIZA, an early natural language processing program.\n",
            "\n",
            "**1966:** Nils Nilsson develops STRIPS, a planning system that enables AI agents to reason about actions and their consequences.\n",
            "\n",
            "**1969:** Marvin Minsky and Seymour Papert publish \"Perceptrons,\" which proves that perceptrons have limited computational power.\n",
            "\n",
            "**1971:** John McCarthy and Nils Nilsson develop the Situation Calculus, a formal language for representing and reasoning about changing situations.\n",
            "\n",
            "**1972:** Raj Reddy develops the Hearsay-II speech recognition system, a significant advance in natural language processing.\n",
            "\n",
            "**1974:** Edward Feigenbaum and Bruce Buchanan develop MYCIN, an expert system for diagnosing infectious diseases.\n",
            "\n",
            "**1979:** The first International Joint Conference on Artificial Intelligence (IJCAI) is held.\n",
            "\n",
            "**1980:** Rodney Brooks introduces the concept of subsumption architecture, a decentralized approach to robotics.\n",
            "\n",
            "**1986:** Geoffrey Hinton, David Rumelhart, and Ronald Williams develop the backpropagation algorithm, which enables the training of multi-layer neural networks.\n",
            "\n",
            "**1997:** IBM's Deep Blue defeats world chess champion Garry Kasparov, demonstrating the power of AI in complex games.\n"
          ]
        }
      ]
    }
  ]
}